# -*- coding: utf-8 -*-
"""final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19JtpVPCnLTWfmLpkbkF93mRECC-ySZHv
"""

#%matplotlib widget

import pandas as pd
import numpy as np

"""# Load CSV Files"""

dataset1 = pd.read_csv("dataset/Challenger_Ranked_Games.csv")
dataset2 = pd.read_csv("dataset/GrandMaster_Ranked_Games.csv")
dataset3 = pd.read_csv("dataset/Master_Ranked_Games.csv")

"""# Combine CSV Files"""

mergeddataset = pd.concat([dataset1,dataset2,dataset3])
mergeddataset.reset_index(drop=True)

"""We will Delete games which is less than 300 seconds."""

mergeddataset = mergeddataset[~(mergeddataset["gameDuraton"] <1200)]

"""we will apply reduntant features technic for 8 columns they will convert to binary presentiotan in 6 column
B = 0, R = 1
- (blueWins, redWins) ->winner
- (blueFirstBlood, redFirstBlood) -> firstBlood
- (blueFirstInhibitor, redFirstInhibitor) -> firstInhibtor 
- (blueFirstBaron, redFirstBaron) -> firstBaron
- (blueFirstDragon, redFirstDragon) -> firstDragon
- (blueFirstTower, redFirstTower) -> firstTower
- (redTotalLevel, redAvgLevel) -> redAvgLevel
- (blueTotalLevel,  blueAvgLevel) -> blueAvgLevel
"""

for x in ['redWins', 'redFirstBlood', 'redFirstInhibitor', 'redFirstBaron', 'redFirstDragon', 'redFirstTower', 'gameId', 'redAvgLevel', 'blueAvgLevel']:
    del mergeddataset[x]

"""X_scaled"""

Y = mergeddataset["blueWins"].values
del mergeddataset["blueWins"]
X = mergeddataset.values

X_columns = mergeddataset.columns

"""normalize data"""

from sklearn import preprocessing
X = mergeddataset.values
min_max_scaler = preprocessing.MinMaxScaler()
X_scaled = min_max_scaler.fit_transform(X)

"""# functions"""

from sklearn.metrics import mean_squared_error
from time import time
def test_model_avg(model_init_func, model_run_func, X_test, y_test, title="Results", simulation_count=5):
    print(f"{title:^110}")
    print("{:<10}{:<40}{:<40}{:<40}".format("#", "Training Time (ms)", "Error (cost)", "Score (%)"))
    print(f"{'-'*110:^110}")
    times = []
    costs = []
    models = []
    scores = []
    for x in range(simulation_count):
        clf = model_init_func()
        start = time()
        model_run_func(clf)
        stop = time()
        y_pred = clf.predict(X_test)
        training_time = stop - start
        error_cost = mean_squared_error(y_test, y_pred)
        score = clf.score(X_test, y_test)
        times += [training_time]
        costs += [error_cost]
        models += [clf]
        scores += [score]
        print(f"{x+1:<10}{training_time * 1000:<40.2f}{error_cost:<40.6f}{score:<40.4f}")
    print(f"{'-'*110:^110}")
    print(f"{'Average':<10}{sum(times)/len(times) * 1000:<40.2f}{sum(costs)/len(costs):<40.6f}{sum(scores)/len(scores):<40.4f}")
    return models

def average_feature_importance(models_features, extract_future_func):
    return list(sorted(list(enumerate(list(map(lambda x: sum(x)/len(models),zip(*map(extract_future_func,models)))))), key=lambda i:i[1], reverse=True))

def calculate_feature_importance(features):
    features = list(features)
    class_1_feature_list = sorted(filter(lambda x: x[1] > 0,features), key=lambda i:i[1], reverse=True)
    class_0_feature_list = sorted(filter(lambda x: x[1] < 0,features),key=lambda i:i[1])
    return class_0_feature_list, class_1_feature_list

import matplotlib.pyplot as plt
def visualize_feature_importance(class_0_feature_list, class_1_feature_list, column_names, feature_count=3, class_count=2):
    fig, axis = plt.subplots(1,class_count)
    fig.tight_layout()
    if not isinstance(axis , np.ndarray):
        axis = [axis]
    fig.set_size_inches((2*class_count) * feature_count, 6)
    fC = list(range(0,feature_count))
    for bar in zip(axis, [class_0_feature_list[:feature_count], class_1_feature_list[:feature_count]], range(class_count)):
        bar[0].set_title(f"Class {bar[2]} Weight Importance")
        bar[0].set_xticks(fC)
        bar[0].set_xticklabels(list(map(lambda x: column_names[x[0]],bar[1])))
        bar[0].bar(fC, list(map(lambda x: abs(x[1]),bar[1])))
    return fig

import matplotlib.pyplot as plt
def visualize_decision_boundary(clf, f1, f2, X_train, y_train, X_test, y_test, X_columns):
    coef = clf.coef_
    c = clf.classes_[0]
    intercept = clf.intercept_
    def line(x0):
      return (-(x0 * coef[c, f1]) - intercept[c]) / coef[c, f2]
    fig, (train_plt, test_plt) = plt.subplots(1, 2)
    fig.tight_layout()
    train_plt.plot(X_train[y_train==0,f1], X_train[y_train==0,f2], 'rx', label='Class0')
    train_plt.plot(X_train[y_train==1,f1], X_train[y_train==1,f2], 'bo', label='Class1')
    train_plt.set_title("Train Data")
    train_plt.set_xlabel(X_columns[f1])
    train_plt.set_ylabel(X_columns[f2])
    test_plt.plot(X_test[y_test==0,f1], X_test[y_test==0,f2], 'rx', label="Class0")
    test_plt.plot(X_test[y_test==1,f1], X_test[y_test==1,f2], 'bo', label="Class1")
    xmin, xmax = test_plt.get_xlim()
    ymin, ymax = test_plt.get_ylim()
    test_plt.plot([xmin, xmax], [line(xmin), line(xmax)], ls="--", color="black", label="hypothesis")
    test_plt.set_title("Test Data")
    test_plt.set_xlabel(X_columns[f1])
    test_plt.set_ylabel(X_columns[f2])
    train_plt.legend()
    test_plt.legend()
    return fig

from sklearn.inspection import permutation_importance
def sckit_permutation_importance(clf,X,y):
    return permutation_importance(clf, X, y).importances_mean

def coef_permutation_importance(x):
    return x.coef_[0]

def print_error_and_ms(ms, error):
    print(f"Average Training Time {ms}, Average Error Cost {error}")

"""## split train and test data"""

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, Y, train_size=0.7)

"""# 3. Linear Classification

## 3.1 Perceptron

### 3.1.1 Performance Analyse
"""

from sklearn.linear_model import Perceptron
models = test_model_avg(lambda: Perceptron(), lambda x: x.fit(X_train, y_train), X_test, y_test, "Linear Classification : Perceptron Results")

"""### 3.1.2 Feature Weight Analyse"""

feature_importances = average_feature_importance(models, lambda x: x.coef_[0])
class_0_feature_list, class_1_feature_list = calculate_feature_importance(feature_importances)
fig = visualize_feature_importance(class_0_feature_list, class_1_feature_list, X_columns)
None
fig.savefig("graphs/perceptron_feature_importance_weights.pdf")

"""### 3.1.3 Feature Analyse with BruteForce"""

feature_importances = average_feature_importance(models, lambda x: sckit_permutation_importance(x,X_test,y_test))
fig = visualize_feature_importance(feature_importances, [], X_columns, 6, 1)
plt.title("Feature Importance")
fig.savefig("graphs/perceptron_feature_importance.pdf")
None

"""### 3.1.4 Decision Boundary Graph"""

fig = visualize_decision_boundary(models[0], feature_importances[0][0], feature_importances[1][0], X_train, y_train, X_test, y_test, X_columns)
fig.savefig("graphs/perceptron_decision_boundary.jpeg")

"""## 3.2 Logistic Regression

### 3.2.1   Performance Analyse
"""

from sklearn.linear_model import LogisticRegression
models = test_model_avg(lambda: LogisticRegression(max_iter=10000), lambda x: x.fit(X_train, y_train), X_test, y_test, "Linear Classification : LogisticRegression Results")

"""### 3.2.2   Feature Weight Analyse"""

feature_importances = average_feature_importance(models, lambda x: x.coef_[0])
class_0_feature_list, class_1_feature_list = calculate_feature_importance(feature_importances)
fig = visualize_feature_importance(class_0_feature_list, class_1_feature_list, X_columns)
None
fig.savefig("graphs/logisticregression_feature_importance_weights.pdf")

"""### 3.2.3   Feature Analyse with BruteForce"""

feature_importances = average_feature_importance(models, lambda x: sckit_permutation_importance(x,X_test,y_test))
fig = visualize_feature_importance(feature_importances, [], X_columns, 6, 1)
plt.title("Feature Importance")
fig.savefig("graphs/logisticregression_feature_importance.pdf")
None

"""### 3.2.4   Decision Boundary Graph"""

fig = visualize_decision_boundary(models[0], class_0_feature_list[0][0], class_1_feature_list[1][0], X_train, y_train, X_test, y_test, X_columns)

fig.savefig("graphs/logisticregression_decision_boundary.jpeg")

"""## 3.3 SGDClassifier

### 3.3.1   Performance Analyse
"""

from sklearn.linear_model import SGDClassifier
models = test_model_avg(lambda: SGDClassifier(), lambda x: x.fit(X_train, y_train), X_test, y_test, "Linear Classification : SGDClassifier Results")

"""### 3.3.2 Feature Weight Analyse"""

feature_importances = average_feature_importance(models, lambda x: x.coef_[0])
class_0_feature_list, class_1_feature_list = calculate_feature_importance(feature_importances)
fig = visualize_feature_importance(class_0_feature_list, class_1_feature_list, X_columns)
None
fig.savefig("graphs/sgdclassifier_feature_importance_weights.pdf")

"""### 3.3.3   Feature Analyse with BruteForce"""

feature_importances = average_feature_importance(models, lambda x: sckit_permutation_importance(x,X_test,y_test))
fig = visualize_feature_importance(feature_importances, [], X_columns, 6, 1)
plt.title("Feature Importance")
fig.savefig("graphs/sgdclassifier_feature_importance.pdf")
None

"""### 3.3.4   Decision Boundary Graph"""

fig = visualize_decision_boundary(models[0], feature_importances[0][0], feature_importances[1][0], X_train, y_train, X_test, y_test, X_columns)
fig.savefig("graphs/sgdclassifier_decision_boundary.jpeg")

"""# 4 Neural Network-based Classification

## 4.1 MLPClassifier

### 4.1.1   Performance Analyse
"""

from sklearn.neural_network import MLPClassifier
models = test_model_avg(lambda: MLPClassifier(), lambda x: x.fit(X_train, y_train), X_test, y_test, "Neural Network-based Classification : MLPClassifier Results", simulation_count=5)

"""### 4.1.3 Feature Analyse with BruteForce"""

feature_importances = average_feature_importance(models, lambda x: sckit_permutation_importance(x,X_test,y_test))
fig = visualize_feature_importance(feature_importances, [], X_columns, 6, 1)
plt.title("Feature Importance")
None
fig.savefig("graphs/mlpclassifier_feature_importance.pdf")

"""# 5 Clustering

## 5.1 KMeans

### 5.1.1 Performance Analyse
"""

from sklearn.cluster import KMeans
models = test_model_avg(lambda: KMeans(n_clusters=2), lambda x: x.fit(X_train, y_train), X_test, y_test, "Clustering : KMeans Results")

"""### 5.1.2 Feature Importance with BruteForce"""

feature_importances = average_feature_importance(models, lambda x: sckit_permutation_importance(x,X_test,y_test))
fig = visualize_feature_importance(feature_importances, [], X_columns, 6, 1)
plt.title("Feature Importance")
fig.savefig("graphs/kmeans_feature_importance.pdf")
None

"""### 5.1.3   Clustering Step by Step"""

from sklearn.cluster import MiniBatchKMeans
km = MiniBatchKMeans(
    n_clusters=2,
    init="random",
    random_state=95)

yp=Y
#visualize
fig, plt_iterations = plt.subplots(3, 3, figsize=(7,5))
plt_iterations = plt_iterations.reshape(9)
fig.set_size_inches(10,10)
for x in range(9):
  km.partial_fit(X)
  yp = km.labels_
  plt_iterations[x].plot(X[yp==0,18], X[yp==0,35], 'rx', markerfacecolor="w")
  plt_iterations[x].plot(X[yp==1,18], X[yp==1,35], 'bx', markerfacecolor="w")
  plt_iterations[x].plot(km.cluster_centers_[:, 18], km.cluster_centers_[:, 35], 'ko')
  plt_iterations[x].set_title(f"Iteration: {x}")
  plt_iterations[x].axis('off')
#plt.close()
None
fig.savefig("graphs/kmeans_clustering_progress.jpeg")

"""#### """